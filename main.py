# -*- coding: utf-8 -*-
"""ML_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r4zutQq_1vikbyJ-K44tidrWYNVaN9zJ
"""

from google.colab import drive
drive.mount("/content/drive")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

weather_AUS=pd.read_csv('/content/drive/MyDrive/weatherAUS.csv')
weather_AUS

weather_AUS.info()

weather_AUS.dropna(subset=['RainToday','RainTomorrow'],inplace=True)

"""EDA and Visualization"""

import plotly.express as px
import matplotlib
import matplotlib.pyplot as plt

px.histogram(weather_AUS,x='Location',title='Location vs. Rainy Days',color='RainToday')

weather_AUS.Location.nunique()

px.histogram(weather_AUS,x='Temp3pm',title='Temp3pm  vs. Rain Tomorrow',color='RainTomorrow')

px.histogram(weather_AUS,x='RainTomorrow',title='Rain Tomorrow vs. Rain Today',color='RainToday')

px.scatter(weather_AUS,title='Min Temp. vs Max Temp.',
x='MinTemp',y='MaxTemp',color='RainToday')

"""Training, Validation and Test Sets"""

from sklearn.model_selection import train_test_split
import seaborn as sns

train_val_df,test_df=train_test_split(weather_AUS,test_size=0.2,random_state=42)
train_df,val_df=train_test_split(train_val_df,test_size=0.25,random_state=42)

print(train_df.shape)
print(val_df.shape)
print(test_df.shape)

plt.title("No. of row per yr")
#plt.figure(figsize=(6,12))
sns.countplot(x=pd.to_datetime(weather_AUS.Date).dt.year);

year=pd.to_datetime(weather_AUS.Date).dt.year
train_df=weather_AUS[year<2015]
val_df=weather_AUS[year==2015]
test_df=weather_AUS[year>2015]

"""
Identifying Input and Target Columns"""

input_colmn=list(train_df.columns)[1:22]
input_colmn
target_colmn = 'RainTomorrow'

train_input=train_df[input_colmn].copy()
train_target=train_df[target_colmn].copy()

val_input=val_df[input_colmn].copy()
val_target=val_df[target_colmn].copy()

test_input=test_df[input_colmn].copy()
test_target=test_df[target_colmn].copy()

train_input

train_target

numeric_colmn = train_input.select_dtypes(include=np.number).columns.tolist()
categorical_colmn = train_input.select_dtypes('object').columns.tolist()

train_input[numeric_colmn].describe()

train_input[categorical_colmn].describe()

"""Imputing Missing Numeric Data"""

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(missing_values=np.nan,strategy='mean')
imputer.fit(weather_AUS[numeric_colmn])

list(imputer.statistics_)#gives avg

imputer.transform(train_input[numeric_colmn])

train_input[numeric_colmn] = imputer.transform(train_input[numeric_colmn])
val_input[numeric_colmn] = imputer.transform(val_input[numeric_colmn])
test_input[numeric_colmn] = imputer.transform(test_input[numeric_colmn])

train_input[numeric_colmn].isna().sum()

"""Scaling Numeric Features by using MinMaxScaler from sklearn.preprocessing"""

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit(weather_AUS[numeric_colmn])

list(scaler.data_min_)

list(scaler.data_max_)

"""scale the training, validation and test sets using the transform method of scaler"""

train_input[numeric_colmn] = scaler.transform(train_input[numeric_colmn])
val_input[numeric_colmn] = scaler.transform(val_input[numeric_colmn])
test_input[numeric_colmn] = scaler.transform(test_input[numeric_colmn])

train_input[numeric_colmn].describe()

val_input[numeric_colmn].describe()

test_input[numeric_colmn].describe()

"""Encoding Categorical Data"""

weather_AUS.Location.unique()

weather_AUS[categorical_colmn].nunique()

from sklearn.preprocessing import OneHotEncoder
encoder=OneHotEncoder(sparse=False,handle_unknown='ignore')

categorical_colmn

encoder.fit(weather_AUS[categorical_colmn])

encoder.categories_

encoded_colmn=list(encoder.get_feature_names(categorical_colmn))
encoded_colmn

train_input[encoded_colmn] = encoder.transform(train_input[categorical_colmn])
val_input[encoded_colmn] = encoder.transform(val_input[categorical_colmn])
test_input[encoded_colmn] = encoder.transform(test_input[categorical_colmn])

test_input

"""saving files"""

train_input.to_parquet('train_input.parquet')
val_input.to_parquet('val_input.parquet')
test_input.to_parquet('test_input.parquet')
pd.DataFrame(train_target).to_parquet('train_target.parquet')
pd.DataFrame(val_target).to_parquet('val_target.parquet')
pd.DataFrame(test_target).to_parquet('test_target.parquet')

"""train the model"""



model=LogisticRegression(solver='liblinear',max_iter=100)

model.fit(train_input[numeric_colmn+encoded_colmn],train_target)

print(numeric_colmn+encoded_colmn)

print(model.coef_.tolist())

weight_df=pd.DataFrame({'feature':numeric_colmn+encoded_colmn,'Weight':model.coef_.tolist()[0]
              })

print(model.intercept_)

plt.figure(figsize=(12,6))
sns.barplot(data=weight_df.sort_values('Weight',ascending=False).head(15),x='Weight',y='feature')

"""Making Predictions and Evaluating the Model"""

x_train=train_input[numeric_colmn+encoded_colmn]
x_val=val_input[numeric_colmn+encoded_colmn]
x_test=test_input[numeric_colmn+encoded_colmn]

train_preds=model.predict(x_train)
train_preds

train_porbs=model.predict_proba(x_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

accuracy_score(train_target, train_preds)

model.classes_

confusion_matrix(train_target, train_preds, normalize='true')

def predict_and_plot(inputs, targets, name=''):
    preds = model.predict(inputs)
    
    accuracy = accuracy_score(targets, preds)
    print("Accuracy: {:.2f}%".format(accuracy * 100))
    cf = confusion_matrix(targets, preds, normalize='true')
    plt.figure()
    sns.heatmap(cf, annot=True)
    plt.xlabel('Prediction')
    plt.ylabel('Target')
    plt.title('{} Confusion Matrix'.format(name));
    
    return preds

train_preds=predict_and_plot(x_train,train_target,'Training')

val_preds = predict_and_plot(x_val, val_target, 'Validatiaon')

test_preds = predict_and_plot(x_test, test_target, 'Test')

def random_guess(inputs):
    return np.random.choice(["No", "Yes"], len(inputs))
def all_no(inputs):
    return np.full(len(inputs), "No")

accuracy_score(test_target, random_guess(x_test))

accuracy_score(test_target, all_no(x_test))

"""Our random model achieves an accuracy of 49.3% and our "always No" model achieves an accuracy of 77.3%.

Thankfully, our model is better than a "dumb" or "random" model! 
"""

def predict_input(input):
    input_df = pd.DataFrame([input])
    input_df[numeric_colmn] = imputer.transform(input_df[numeric_colmn])
    input_df[numeric_colmn] = scaler.transform(input_df[numeric_colmn])
    input_df[encoded_colmn] = encoder.transform(input_df[categorical_colmn])
    x_input = input_df[numeric_colmn + encoded_colmn]
    pred = model.predict(x_input)[0]
    prob = model.predict_proba(x_input)[0][list(model.classes_).index(pred)]
    return pred, prob

input_1 = {'Date': '2021-06-19',
             'Location': 'Launceston',
             'MinTemp': 23.2,
             'MaxTemp': 23,
             'Rainfall': 10.2,
             'Evaporation': 4.2,
             'Sunshine': np.nan,
             'WindGustDir': 'NNW',
             'WindGustSpeed': 52.0,
             'WindDir9am': 'NW',
             'WindDir3pm': 'NNE',
             'WindSpeed9am': 13.0,
             'WindSpeed3pm': 20.0,
             'Humidity9am': 89.0,
             'Humidity3pm': 58.0,
             'Pressure9am': 1004.8,
             'Pressure3pm': 1001.5,
             'Cloud9am': 8.0,
             'Cloud3pm': 5.0,
             'Temp9am': 25.7,
             'Temp3pm': 33.0,
             'RainToday': 'No'}

predict_input(input_1)

"""Saving Trained Models"""

import joblib


aussie_rain = {
    'model': model,
    'imputer': imputer,
    'scaler': scaler,
    'encoder': encoder,
    'input_colmn': input_colmn,
    'target_colmn': target_colmn,
    'numeric_colmn': numeric_colmn,
    'categorical_colmn': categorical_colmn,
    'encoded_colmn': encoded_colmn
}

joblib.dump(aussie_rain,'aussie_rain.joblib')